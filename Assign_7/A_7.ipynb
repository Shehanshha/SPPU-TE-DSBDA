{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e5c1160b-9c10-492e-ab38-d38df4690a35",
   "metadata": {},
   "source": [
    "NLP- nlp is a branch of a ai that help to computer to read,understand, interpret, and manuplate human langauges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25090594-af48-4c25-9f46-ffb0e5f13f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\python311\\lib\\site-packages (3.8.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: click in c:\\python311\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\python311\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\python311\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\python311\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5685b9bd-497a-40a4-ad1e-0a492c26d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer,MWETokenizer,TweetTokenizer,WordPunctTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccfc1f34-b8df-4581-b437-64c0663c0dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d150e163-1cdc-48b0-ad27-30b57fb9a0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a sample sentence. And here's another one, ðŸ˜€ðŸ¥³ with a multi-word expression like New York City.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b908b909-5528-4934-9fee-238c73262f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace Tokenization: ['This', 'is', 'a', 'sample', 'sentence.', 'And', \"here's\", 'another', 'one,', 'ðŸ˜€ðŸ¥³', 'with', 'a', 'multi-word', 'expression', 'like', 'New', 'York', 'City.']\n",
      "Whitespace Token Count: 18\n"
     ]
    }
   ],
   "source": [
    "\n",
    "whitespace_tokenizer = WhitespaceTokenizer()\n",
    "whitespace_tokens = whitespace_tokenizer.tokenize(text)\n",
    "print(\"Whitespace Tokenization:\", whitespace_tokens)\n",
    "whitespace_token_count = len(whitespace_tokens)\n",
    "print(\"Whitespace Token Count:\", whitespace_token_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b356c62f-278c-44ce-a4fe-4c459af71198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MWE Tokenization: ['This', 'is', 'a', 'sample', 'sentence', '.', 'And', 'here', \"'s\", 'another', 'one', ',', 'ðŸ˜€ðŸ¥³', 'with', 'a', 'multi-word', 'expression', 'like', 'New_York_City', '.']\n",
      "MWE Token Count: 20\n"
     ]
    }
   ],
   "source": [
    "mwe_tokenizer = MWETokenizer([('New', 'York', 'City')])\n",
    "mwe_tokens = mwe_tokenizer.tokenize(nltk.word_tokenize(text))\n",
    "print(\"MWE Tokenization:\", mwe_tokens)\n",
    "mwe_token_count = len(mwe_tokens)\n",
    "print(\"MWE Token Count:\", mwe_token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28d5f57e-9532-463c-b85c-8502ed533287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation-based Tokenization: ['This', 'is', 'a', 'sample', 'sentence', '.', 'And', 'here', \"'\", 's', 'another', 'one', ',', 'ðŸ˜€ðŸ¥³', 'with', 'a', 'multi', '-', 'word', 'expression', 'like', 'New', 'York', 'City', '.']\n",
      "Punctuation Token Count: 25\n"
     ]
    }
   ],
   "source": [
    "punct_tokenizer = WordPunctTokenizer()\n",
    "punct_tokens = punct_tokenizer.tokenize(text)\n",
    "print(\"Punctuation-based Tokenization:\", punct_tokens)\n",
    "punctuation_token_count = len(punct_tokens)\n",
    "print(\"Punctuation Token Count:\", punctuation_token_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4000837-b98d-4e5e-bd9b-377d46eb05b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet-based Tokenization: ['This', 'is', 'a', 'sample', 'sentence', '.', 'And', \"here's\", 'another', 'one', ',', 'ðŸ˜€', 'ðŸ¥³', 'with', 'a', 'multi-word', 'expression', 'like', 'New', 'York', 'City', '.']\n",
      "Tweep Token Count: 22\n"
     ]
    }
   ],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokens = tweet_tokenizer.tokenize(text)\n",
    "print(\"Tweet-based Tokenization:\", tweet_tokens)\n",
    "tweep_token_count = len(tweet_tokens)\n",
    "print(\"Tweep Token Count:\", tweep_token_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396ff876-f0e3-4abe-bfa4-8ccca9f49233",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ed7f11c-75ed-4cee-9864-fd2c6bbe31ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "words = ['wait','waiting','waited','waits']\n",
    "ps = PorterStemmer()\n",
    "for w in words:\n",
    "    word = ps.stem(w)\n",
    "print(word)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f1f6a6-e774-4c47-a281-99ce325fc590",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8eae483e-7229-4f2d-a5a4-b138868e4153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for studies is study\n",
      "Lemma for studying is studying\n",
      "Lemma for cries is cry\n",
      "Lemma for cry is cry\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "word = WordNetLemmatizer()\n",
    "text ='studies studying cries cry'\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} is {}\".format(w,word.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4601a16-bc78-4836-954c-6e45b26dd413",
   "metadata": {},
   "source": [
    "# POS Tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70aae3ee-12ed-4aaa-9a4c-141ef157f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DT\n",
      "pink NN\n",
      "sweater NN\n",
      "fit VBP\n",
      "her PRP$\n",
      "perfectly RB\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "data = \"The pink sweater fit her perfectly\"\n",
    "tokens = word_tokenize(data)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "for word, pos_tag in pos_tags:\n",
    "    print(word, pos_tag)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a5a082d-7bb5-4f3e-90be-a826ddcfa3af",
   "metadata": {},
   "source": [
    "'DT': Determiner\n",
    "'NN': Noun, singular or mass\n",
    "'VBP' : verb\n",
    "'PRP$': Possessive pronoun\n",
    "'RB': Adverb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d86c7f-d2d9-4371-a6fd-25fed5b2e3aa",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad8d564c-3a70-4751-9145-836acfeae472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ai', 'introduced', 'year', '1956', 'gained', 'popularity', 'recently', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "data = \"Ai was introduced in the year 1956 but it gained popularity recently.\"\n",
    "stopwords = set(stopwords.words('english'))\n",
    "words = word_tokenize(data)\n",
    "wordfilterd = []\n",
    "\n",
    "for w in words:\n",
    "    if w not in stopwords:\n",
    "        wordfilterd.append(w)\n",
    "print(wordfilterd)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a26e287-e028-4fbb-b625-d33e7436f435",
   "metadata": {},
   "source": [
    "# TF -term frrequency \n",
    "# IDF - inverse term frequency"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d049f5ef-0c8a-45ea-a4cb-299ef515fd0b",
   "metadata": {},
   "source": [
    "TF- how many times tem occurs in a document\n",
    "IDF - it finds rare words in a document"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a516881c-9e68-4eff-aff2-786f57f2581d",
   "metadata": {},
   "source": [
    "      (No.of times term t appers in a document)                       Total no.of documents\n",
    "TF = ---------------------------------------------      IDF =    log( ------------------------           )\n",
    "      total no.of term in the document                                no.of documents with term t in it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f78168c1-bbde-4e76-bac2-8e5467f0b560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf8ab565-39c6-4940-bbe3-5558fa0430a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "doc1 = \"Jupiter is the largest Planet\"\n",
    "doc2 = \"Mars is the fourth planet from the Sun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c0ce736-238c-4b79-a8cf-dd8cd5d9e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tfidf.fit_transform([doc1,doc2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee80117e-d14c-4b63-8a79-0f3db51c2343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(tfidf.vocabulary_)) #This will print the number of unique terms in the vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0d2eed4-998e-473f-83f0-b6bec62c62cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jupiter': 3,\n",
       " 'is': 2,\n",
       " 'the': 8,\n",
       " 'largest': 4,\n",
       " 'planet': 6,\n",
       " 'mars': 5,\n",
       " 'fourth': 0,\n",
       " 'from': 1,\n",
       " 'sun': 7}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_  #convert words in number due- algorithm understand numrical data eaisly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45ed2db8-c341-44b7-95c4-9ddce64489ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t0.3793034928087496\n",
      "  (0, 4)\t0.5330978245262535\n",
      "  (0, 8)\t0.3793034928087496\n",
      "  (0, 2)\t0.3793034928087496\n",
      "  (0, 3)\t0.5330978245262535\n",
      "  (1, 7)\t0.37695708675831013\n",
      "  (1, 1)\t0.37695708675831013\n",
      "  (1, 0)\t0.37695708675831013\n",
      "  (1, 5)\t0.37695708675831013\n",
      "  (1, 6)\t0.2682080718928097\n",
      "  (1, 8)\t0.5364161437856194\n",
      "  (1, 2)\t0.2682080718928097\n"
     ]
    }
   ],
   "source": [
    "print(response) #got values for hole vocubulary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
